# -*- coding: utf-8 -*-
"""Chatbot Webapp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zH-MmYSNxpJZGIb4kff5FeoWh_zyWMmP
"""

!pip install openai
!pip install streamlit
!pip install langchain
!pip install chromadb
!pip install tiktoken
!pip install -q streamlit
!pip install pypdf

import os
import csv
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import CSVLoader
import streamlit as st
from openai.error import OpenAIError
from langchain.vectorstores.faiss import FAISS
from openai.error import AuthenticationError
from pypdf import PdfReader
from io import BytesIO

from google.colab import drive
drive.mount('/content/drive')

# os.environ["OPENAI_API_KEY"] = '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'

# persist_directory = "drive/My Drive/"
# csv_path = 'drive/My Drive/temp.csv'

persist_directory = ""
csv_path = '/temp.csv'

# csv_path = 'drive/My Drive/data job posts.csv'

#openai.api_key = os.getenv("OPENAI_API_KEY") or st.secrets["OPENAI_API_KEY"]

def set_openai_api_key(api_key: str):
    st.session_state["OPENAI_API_KEY"] = api_key


def sidebar():
    with st.sidebar:
        if api_key_input:
            set_openai_api_key(api_key_input)

        st.markdown("---")
        st.markdown("# About")
        st.markdown(
            "JobGPT allows you to match the best job with your preference "
            "and resume. "
        )
        st.markdown(
            "JobGPT used a job listing dataset from 2016, so it is not real time updated. This product is still a work in progress..."
            "Feel free to improve this product GITHUB LINK"
        )
        st.markdown("Made by [mmz_001](https://www.linkedin.com/in/chuheng-yu99/)")
        st.markdown("---")

def parse_pdf(file: BytesIO) -> list[str]:
    pdf = PdfReader(file)
    output = []
    for page in pdf.pages:
        text = page.extract_text()
        # Merge hyphenated words
        text = re.sub(r"(\w+)-\n(\w+)", r"\1\2", text)
        # Fix newlines in the middle of sentences
        text = re.sub(r"(?<!\n\s)\n(?!\s\n)", " ", text.strip())
        # Remove multiple newlines
        text = re.sub(r"\n\s*\n", "\n\n", text)

        output.append(text)

    return output

def remove_null_bytes(file_path, encoding='utf-8'):
    with open(file_path, 'rb') as f:
        content = f.read()
    content = content.replace(b'\x00', b'')  # Remove null bytes
    with open(file_path, 'wb') as f:
        f.write(content)

def embed_docs(text):
  if not st.session_state.get("OPENAI_API_KEY"):
        raise AuthenticationError(
            "Enter your OpenAI API key in the sidebar. You can get a key at"
            " https://platform.openai.com/account/api-keys."
        )
  else:
        # Embed the chunks
        embeddings = OpenAIEmbeddings(
            openai_api_key=st.session_state.get("OPENAI_API_KEY")
        )  # type: ignore
        index = FAISS.from_documents(text, embeddings)

        return index

def text_to_docs(text):
    """Converts a string or list of strings to a list of Documents
    with metadata."""
    if isinstance(text, str):
        # Take a single string as one page
        text = [text]
    page_docs = [Document(page_content=page) for page in text]

    # Add page numbers as metadata
    for i, doc in enumerate(page_docs):
        doc.metadata["page"] = i + 1

    # Split pages into chunks
    doc_chunks = []

    for doc in page_docs:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            chunk_overlap=0,
        )
        chunks = text_splitter.split_text(doc.page_content)
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk, metadata={"page": doc.metadata["page"], "chunk": i}
            )
            doc_chunks.append(doc)
    return doc_chunks

def clear_submit():
    st.session_state["submit"] = False

st.set_page_config(page_title="JobGPT", page_icon="ðŸ‘¨â€ðŸ’»", layout="wide")
st.header("JobGPT")

uploaded_file = st.file_uploader(
    "Upload a pdf version of your resume",
    type=["pdf"],
    on_change=clear_submit,
)

resume = st.text_input(label="Or paste your resume here", placeholder="AI")

st.markdown(
            "## How to use\n"
            "1. Enter your [OpenAI API key](https://platform.openai.com/account/api-keys) belowðŸ”‘\n"  # noqa: E501
            "2. Upload a docx, or txt resumeðŸ“„\n"
            "3. Enter a job position you wantðŸ’¬\n"
        )
api_key_input = st.text_input(
            "OpenAI API Key",
            type="password",
            placeholder="Paste your OpenAI API key here (sk-...)",
            help="You can get your API key from https://platform.openai.com/account/api-keys.",  # noqa: E501
            value=st.session_state.get("OPENAI_API_KEY", ""),
        )

sidebar()

index = None
doc = None
if uploaded_file is not None and resume is None:
    if uploaded_file.name.endswith(".pdf"):
        doc = parse_pdf(uploaded_file)
    else:
        raise ValueError("File type not supported!")
    text = text_to_docs(doc)
    try:
        with st.spinner("Indexing document... This may take a whileâ³"):
            index = embed_docs(text)
        st.session_state["api_key_configured"] = True
    except OpenAIError as e:
        st.error(e._message)
else:
  text = text_to_docs(resume)
  try:
        with st.spinner("Indexing document... This may take a whileâ³"):
            index = embed_docs(text)
        st.session_state["api_key_configured"] = True
  except OpenAIError as e:
        st.error(e._message)


query = st.text_area("Enter your job preference(title, location, hybrid, etc.)", on_change=clear_submit)

button = st.button("Submit")
if button or st.session_state.get("submit"):
    if not st.session_state.get("api_key_configured"):
        st.error("Please configure your OpenAI API key!")
    elif not index:
        st.error("Please upload the resume!")
    elif not query:
        st.error("Please enter your job preference!")
    else:
        st.session_state["submit"] = True


        #job dataset indexing, etc.
        remove_null_bytes(csv_path, encoding='utf-8')
        csv_loader = CSVLoader(csv_path, encoding='utf-8')
        documents = csv_loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=10)
        texts = text_splitter.split_documents(documents)

        embeddings = OpenAIEmbeddings(openai_api_key=st.session_state.get("OPENAI_API_KEY"))
        vectordb = Chroma.from_documents(documents=texts,
                                        embedding=embeddings,
                                        persist_directory=persist_directory)
        vectordb.persist()

        retriever = vectordb.as_retriever(search_kwargs={"k": 3})
        llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=st.session_state.get("OPENAI_API_KEY"))

        qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
        ###

        answer_col = st.columns(1)
        try:
          llm_response = qa(query)
          answer = llm_response["result"]

          with answer_col:
            st.markdown("Answer")
            st.markdown(answer)

        except OpenAIError as e:
            st.error(e._message)